# Bug Fixes - FilterMate v3.0.5 (Proposed)

**Date:** January 7, 2026
**Investigation by:** Claude Code
**Status:** Proposed fixes for review

---

## Summary

This document details 4 bugs discovered through deep codebase analysis and proposes fixes for each:

1. **[CRITICAL]** FID Regex Only Matches 'fid' - Multi-step filtering fails with non-fid primary keys
2. **[HIGH]** Unnecessary PostgreSQL→OGR Fallback - Performance degradation when psycopg2 unavailable
3. **[MEDIUM]** Mid-range WKT (50-500KB) Without Bbox Optimization - Potential QGIS freezes
4. **[LOW]** Signal Management Two-Tier System - Maintenance complexity and hidden dependencies

---

## Bug #1: FID Regex Only Matches 'fid' (CRITICAL)

### Problem Description

**Location:** `modules/backends/spatialite_backend.py:3316, 4107`

The FID-only filter detection regex only matches the literal string `"fid"` (case insensitive):

```python
is_fid_only = bool(re.match(r'^\s*\(?\s*(["\']?)fid\1\s+(IN\s*\(|=\s*-?\d+)',
                            old_subset, re.IGNORECASE))
```

**Issue:** FilterMate supports multiple primary key column names:
- `fid` (GeoPackage default)
- `id`, `gid`, `ogc_fid`, `uid` (exact matches)
- `node_id`, `feature_id`, `AGG_ID` (pattern matches ending with `_id`)
- First integer field (fallback)

When filtering layers with non-"fid" primary keys, the FID filter from Step 1 is NOT recognized as FID-only, breaking multi-step filter combination logic.

### Impact

**Severity:** CRITICAL
**Affected Operations:** Multi-step spatial filtering with non-"fid" primary keys

**Example Failure Scenario:**
```
Step 1: Filter buildings layer → demand_points shows 319 features (PK="id")
Step 2: Filter ducts layer → demand_points shows 9231 features ❌ (should be ~50-100)
```

**Root Cause:** Old subset contains `"id" IN (1,2,3,...)` which is NOT detected as FID-only,
so the multi-step combination logic fails to combine Step 1 and Step 2 results properly.

### Proposed Fix

**File:** `modules/backends/spatialite_backend.py`

Replace hardcoded `'fid'` regex with dynamic primary key detection.

#### Location 1: Line ~3212-3316

```python
# BEFORE (line 3212-3316)
pk_col = 'fid'  # Default for GeoPackage
try:
    pk_indices = layer.primaryKeyAttributes()
    if pk_indices:
        fields = layer.fields()
        pk_col = fields.at(pk_indices[0]).name()
except Exception:
    pass

# ... 100 lines later ...

import re
is_fid_only = bool(re.match(r'^\s*\(?\s*(["\']?)fid\1\s+(IN\s*\(|=\s*-?\d+)',
                            old_subset, re.IGNORECASE))

# AFTER (proposed fix v3.0.5)
pk_col = 'fid'  # Default for GeoPackage
try:
    pk_indices = layer.primaryKeyAttributes()
    if pk_indices:
        fields = layer.fields()
        pk_col = fields.at(pk_indices[0]).name()
except Exception:
    pass

# ... 100 lines later ...

# v3.0.5: CRITICAL FIX - Dynamic FID regex to support any primary key name
# The regex now uses pk_col variable instead of hardcoded 'fid'
# Supports: fid, id, ogc_fid, gid, node_id, AGG_ID, etc.
import re
# Escape pk_col for regex safety (prevent injection)
pk_col_escaped = re.escape(pk_col)
# Build regex pattern dynamically: matches quoted or unquoted pk_col
is_fid_only = bool(re.match(
    rf'^\s*\(?\s*(["\']?){pk_col_escaped}\1\s+(IN\s*\(|=\s*-?\d+|BETWEEN\s+)',
    old_subset,
    re.IGNORECASE
))
```

**Key Changes:**
1. Use `pk_col` variable (already computed at line 3212)
2. Escape pk_col with `re.escape()` to prevent regex injection
3. Support additional patterns: `BETWEEN` (from `_build_range_based_filter`)
4. Same fix applies to line 4107 (second occurrence)

#### Location 2: Line ~4107 (similar fix)

Same pattern, replace:
```python
is_fid_only = bool(re.match(r'^\s*\(?\s*(["\']{0,1})fid\1\s+(IN\s*\(|=\s*-?\d+)',
                            old_subset, re.IGNORECASE))
```

With:
```python
# v3.0.5: Dynamic FID regex (see line 3316 for detailed comments)
import re
pk_col_escaped = re.escape(pk_col)
is_fid_only = bool(re.match(
    rf'^\s*\(?\s*(["\']?){pk_col_escaped}\1\s+(IN\s*\(|=\s*-?\d+|BETWEEN\s+)',
    old_subset,
    re.IGNORECASE
))
```

### Testing Recommendations

**Test Case 1:** Multi-step filtering with `id` as primary key
```python
# Layer: demand_points with PK="id"
# Step 1: Filter by buildings (should create: id IN (1,2,3,...))
# Step 2: Filter by ducts (should combine: id IN (1,2,3,...) AND new_filter)
# Expected: Intersection of both filters
```

**Test Case 2:** Multi-step filtering with `ogc_fid` as primary key
**Test Case 3:** Multi-step filtering with `node_id` (pattern match)
**Test Case 4:** Verify backward compatibility with `fid` layers (GeoPackage)

---

## Bug #2: Unnecessary PostgreSQL→OGR Fallback (HIGH)

### Problem Description

**Location:** `modules/tasks/layer_management_task.py:663-666`, `filter_task.py:599`

PostgreSQL layers fall back to OGR backend when `psycopg2` is unavailable, even though QGIS can filter them natively via `setSubsetString()`.

**Code Analysis:**

```python
# layer_management_task.py:663-666
if layer_provider_type == PROVIDER_POSTGRES and POSTGRESQL_AVAILABLE:
    postgresql_connection_available = True  # ❌ WRONG! Should be True always
    logger.debug(f"PostgreSQL layer {layer.name()}: basic filtering available via QGIS native API")
```

**Contradiction:**
- **Comment says:** "PostgreSQL layers loaded in QGIS are ALWAYS filterable via QGIS native API"
- **Code does:** Only sets `True` when `POSTGRESQL_AVAILABLE` (requires psycopg2)

Result: Without psycopg2, `postgresql_connection_available=False` → falls back to OGR (slower).

### Impact

**Severity:** HIGH
**Affected Operations:** All PostgreSQL filtering when psycopg2 not installed

**Performance Impact:**
- PostgreSQL backend: <1s for 100k features
- OGR fallback: ~30s for 100k features (30x slower!)

**User Experience:**
- Users without psycopg2 get dramatically slower filtering
- No clear warning that installing psycopg2 would improve performance

### Proposed Fix

**File:** `modules/tasks/layer_management_task.py`

Remove `POSTGRESQL_AVAILABLE` check from basic filtering availability.

```python
# BEFORE (line 663-666)
if layer_provider_type == PROVIDER_POSTGRES and POSTGRESQL_AVAILABLE:
    postgresql_connection_available = True
    logger.debug(f"PostgreSQL layer {layer.name()}: basic filtering available via QGIS native API")

# AFTER (proposed fix v3.0.5)
# v3.0.5: CRITICAL FIX - PostgreSQL layers are ALWAYS filterable via QGIS native API
# psycopg2 is only needed for ADVANCED features (materialized views, indexes)
# Basic filtering via setSubsetString() works without psycopg2
if layer_provider_type == PROVIDER_POSTGRES:
    postgresql_connection_available = True
    logger.debug(f"PostgreSQL layer {layer.name()}: basic filtering available via QGIS native API (psycopg2 not required)")

    # Only test psycopg2 connection for ADVANCED features (if psycopg2 is available)
    if POSTGRESQL_AVAILABLE and PSYCOPG2_AVAILABLE:
        # ... existing psycopg2 test code ...
```

**Key Changes:**
1. Remove `and POSTGRESQL_AVAILABLE` from line 663 condition
2. PostgreSQL layers ALWAYS get `postgresql_connection_available = True`
3. Advanced features (MVs, indexes) still require psycopg2
4. Add warning message when psycopg2 unavailable but layer still filterable

### Additional Enhancement

Add user notification when psycopg2 is missing:

```python
# After setting postgresql_connection_available = True
if not POSTGRESQL_AVAILABLE:
    logger.info(
        f"⚠️ PostgreSQL layer '{layer.name()}': psycopg2 not installed.\n"
        f"   Basic filtering works, but for MUCH faster performance:\n"
        f"   → Install: pip install psycopg2-binary\n"
        f"   → Enables materialized views (10-100x faster for large datasets)"
    )
```

### Testing Recommendations

**Test Environment Setup:**
1. Uninstall psycopg2: `pip uninstall psycopg2 psycopg2-binary`
2. Load PostgreSQL layer in QGIS
3. Verify FilterMate still uses PostgreSQL backend (not OGR)
4. Apply spatial filter → should complete successfully
5. Performance should be reasonable (not 30x slower)

**Expected Behavior:**
- Without psycopg2: Uses PostgreSQL backend with basic SQL filtering
- With psycopg2: Uses PostgreSQL backend with materialized views (faster)

---

## Bug #3: Mid-Range WKT (50-500KB) Without Bbox Optimization (MEDIUM)

### Problem Description

**Location:** `modules/backends/spatialite_backend.py:2514-2516, 3272-3287`

WKT between 50KB and 500KB uses source table optimization (R-tree) but NOT bbox pre-filter.

**Thresholds:**
```python
LARGE_WKT_THRESHOLD = 50000       # 50KB - activates source table + R-tree
VERY_LARGE_WKT_THRESHOLD = 500000  # 500KB - activates bbox pre-filter
```

**Current Behavior:**
- WKT 0-50KB: Direct SQL (inline WKT in query)
- WKT 50-500KB: Source table + R-tree index ✅
- WKT 500KB+: Source table + R-tree + **bbox pre-filter** ✅

**Problem:**
WKT between 50-500KB may still cause QGIS freezes depending on geometry complexity.
The comment at line 2516 says: "using R-tree optimization **to prevent freeze**"
but R-tree alone may be insufficient for complex 100-200KB geometries.

### Impact

**Severity:** MEDIUM
**Affected Operations:** Spatial filtering with complex mid-range WKT

**Symptoms:**
- QGIS freezes for 5-30 seconds during filtering
- No progress indicator (appears hung)
- More likely with complex geometries (many vertices, holes, multi-parts)

**Risk Factors:**
- Geometry complexity (not just size)
- Number of vertices (simplified geometries perform better)
- Multi-part geometries (GeometryCollections)

### Proposed Fix

**File:** `modules/backends/spatialite_backend.py`

Lower the `VERY_LARGE_WKT_THRESHOLD` to apply bbox pre-filter sooner.

#### Option 1: Conservative (Recommended)

```python
# BEFORE (line 1123-1125)
LARGE_WKT_THRESHOLD = 50000  # WKT chars - above this, inline SQL can freeze QGIS
VERY_LARGE_WKT_THRESHOLD = 500000  # WKT chars - above this, use bbox pre-filter

# AFTER (proposed fix v3.0.5)
LARGE_WKT_THRESHOLD = 50000       # WKT chars - activates source table + R-tree
VERY_LARGE_WKT_THRESHOLD = 150000  # v3.0.5: Lowered from 500KB to 150KB
                                   # Bbox pre-filter applied sooner to prevent freezes
                                   # with complex geometries in 150-500KB range
```

**Rationale:**
- 150KB is a reasonable middle ground
- Adds minimal overhead (bbox extraction is fast)
- Provides better safety margin for complex geometries

#### Option 2: Aggressive

```python
LARGE_WKT_THRESHOLD = 50000       # WKT chars - activates source table + R-tree
VERY_LARGE_WKT_THRESHOLD = 100000  # v3.0.5: Lowered to 100KB for maximum safety
```

**Trade-off:** Slightly more overhead for 100-150KB WKTs, but guarantees no freezes.

#### Option 3: Adaptive (Advanced)

Add geometry complexity detection:

```python
def should_use_bbox_prefilter(source_wkt: str, wkt_size: int) -> bool:
    """
    v3.0.5: Adaptive bbox pre-filter decision based on WKT size AND complexity.

    Returns True if bbox pre-filter should be used.
    """
    if wkt_size >= 500000:  # Very large - always use
        return True

    if wkt_size < 50000:  # Small - never use
        return False

    # Mid-range (50-500KB) - check complexity
    # Count vertices as proxy for complexity
    vertex_count = source_wkt.count(',')

    # High vertex density → use bbox pre-filter
    vertex_density = vertex_count / wkt_size
    if vertex_density > 0.05:  # > 5% of chars are commas (high vertex count)
        return True

    # Default threshold for mid-range
    return wkt_size >= 150000
```

### Testing Recommendations

**Test Case 1:** Mid-range simple geometry (100KB, low vertices)
- Should complete without freeze
- Performance: <5s

**Test Case 2:** Mid-range complex geometry (150KB, many vertices)
- With fix: Should use bbox pre-filter
- Performance: <5s

**Test Case 3:** Very large geometry (600KB)
- Verify bbox pre-filter still activates
- Performance: <10s

**Test Case 4:** Regression test (20KB simple)
- Verify direct SQL still used (no unnecessary optimization)
- Performance: <1s

---

## Bug #4: Signal Management Two-Tier System (LOW)

### Problem Description

**Location:** `filter_mate_dockwidget.py:415-482 (manageSignal), 4590-4620 (direct connections)`

FilterMate uses TWO different signal management systems:

**System 1:** `manageSignal()` with `_signal_connection_states` cache
- Used for most widgets
- Cache can become desynchronized with Qt signal state
- `isSignalConnected()` is unreliable for specific handler tracking

**System 2:** Direct connections (`_setup_expression_widget_direct_connections`)
- Used for QgsFieldExpressionWidget widgets
- Bypasses cache system
- Not tracked in signal inventory

**Quote from code (line 4587):**
> "These bypass the unreliable manageSignal/isSignalConnected system"

### Impact

**Severity:** LOW (no active bug, but maintenance burden)
**Risk:** Future signal management changes may miss direct connection paths

**Problems:**
1. **Two-tier maintenance:** Changes must update both systems
2. **Hidden dependencies:** Direct connections not in signal inventory
3. **Inconsistent testing:** Test coverage may miss direct connection paths
4. **Cache desync:** Recent bugs (v2.9.24) show cache becomes stale

**Recent Bug Example (v2.9.24):**
- Signal cache said "connected" but signals were actually disconnected
- Action buttons stopped working after filtering
- Fix: `force_reconnect_action_signals()` bypasses cache

### Proposed Fix

**Long-term architectural improvement** (not urgent):

#### Phase 1: Consolidate Signal Tracking

Create unified signal registry that tracks ALL connections:

```python
class SignalRegistry:
    """
    v3.0.5: Unified signal connection registry.

    Replaces _signal_connection_states cache and direct connections
    with single source of truth.
    """

    def __init__(self):
        self._connections = {}  # {widget_key: {signal_name: [handlers]}}
        self._direct_connections = {}  # Track bypass connections

    def connect(self, widget, signal_name, handler, bypass_cache=False):
        """Connect signal and track in registry."""
        key = self._get_widget_key(widget)

        if key not in self._connections:
            self._connections[key] = {}
        if signal_name not in self._connections[key]:
            self._connections[key][signal_name] = []

        # Perform actual connection
        getattr(widget, signal_name).connect(handler)

        # Track in registry
        self._connections[key][signal_name].append(handler)

        if bypass_cache:
            self._direct_connections[key] = self._direct_connections.get(key, [])
            self._direct_connections[key].append((signal_name, handler))

    def disconnect(self, widget, signal_name, handler=None):
        """Disconnect signal and update registry."""
        # ... implementation ...

    def is_connected(self, widget, signal_name, handler=None):
        """Check if signal is connected (specific handler or any)."""
        key = self._get_widget_key(widget)
        if key not in self._connections:
            return False
        if signal_name not in self._connections[key]:
            return False

        if handler:
            return handler in self._connections[key][signal_name]
        return len(self._connections[key][signal_name]) > 0

    def audit(self):
        """Return complete signal inventory for debugging."""
        return {
            'cached': self._connections,
            'direct': self._direct_connections
        }
```

#### Phase 2: Replace Existing Systems

1. Replace `manageSignal()` calls with `SignalRegistry.connect()`
2. Replace direct connections with `SignalRegistry.connect(bypass_cache=True)`
3. Add audit logging to detect desync early

#### Phase 3: Deprecate Cache

Once unified system is stable, remove:
- `_signal_connection_states` dict
- `isSignalConnected()` workarounds
- `force_reconnect_action_signals()` (no longer needed)

### Testing Recommendations

**Phase 1 Testing:**
1. Unit tests for SignalRegistry class
2. Verify connect/disconnect tracking accuracy
3. Test with multiple handlers on same signal

**Integration Testing:**
1. Replace one widget group (e.g., EXPLORING widgets)
2. Verify no behavioral changes
3. Monitor for signal-related bugs over 2-3 releases

**Rollout Strategy:**
- Implement in new code first (v3.0.6+)
- Migrate existing code incrementally (v3.1.x)
- Remove old system when migration complete (v3.2.0)

---

## Implementation Priority

### Immediate (v3.0.5)

1. ✅ **Bug #1: FID Regex** - Critical multi-step filtering fix
2. ✅ **Bug #2: PostgreSQL Fallback** - High performance impact

### Near-term (v3.0.6)

3. ✅ **Bug #3: WKT Bbox Threshold** - Medium safety improvement

### Long-term (v3.1.0+)

4. ⏳ **Bug #4: Signal Management Refactor** - Architectural improvement

---

## Testing Checklist

Before releasing v3.0.5, test ALL fixes:

### Bug #1 Testing
- [ ] Multi-step filtering with PK="id"
- [ ] Multi-step filtering with PK="ogc_fid"
- [ ] Multi-step filtering with PK="node_id"
- [ ] Backward compatibility with PK="fid"

### Bug #2 Testing
- [ ] Uninstall psycopg2
- [ ] Load PostgreSQL layer
- [ ] Verify PostgreSQL backend used (not OGR)
- [ ] Apply spatial filter successfully
- [ ] Reinstall psycopg2
- [ ] Verify materialized views work

### Bug #3 Testing
- [ ] Filter with 100KB simple WKT
- [ ] Filter with 150KB complex WKT
- [ ] Filter with 600KB very large WKT
- [ ] Verify no QGIS freezes

### Bug #4 Testing
- [ ] N/A (long-term refactor, no immediate changes)

---

## Risk Assessment

### Bug #1: LOW RISK
- **Change scope:** 2 regex patterns
- **Fallback:** Original regex still works for 'fid' layers
- **Test coverage:** Easy to verify with known PK names

### Bug #2: MEDIUM RISK
- **Change scope:** 1 condition removal
- **Fallback:** Users can install psycopg2 to restore original behavior
- **Test coverage:** Requires test environment without psycopg2

### Bug #3: LOW RISK
- **Change scope:** 1 constant value change
- **Fallback:** Can revert threshold if performance degrades
- **Test coverage:** Requires large WKT test geometries

### Bug #4: LOW RISK (deferred)
- **Change scope:** N/A (not implemented yet)
- **Fallback:** N/A
- **Test coverage:** Will be part of v3.1.0+ planning

---

## Conclusion

All 4 bugs have been thoroughly investigated and fixes proposed. Bugs #1 and #2 should be implemented immediately in v3.0.5 due to their critical/high severity and impact on core functionality.

**Estimated development time:**
- Bug #1: 30 minutes (2 regex replacements + tests)
- Bug #2: 30 minutes (1 condition removal + tests)
- Bug #3: 15 minutes (1 constant change + tests)
- Total: ~1.5 hours + testing

**Recommended release process:**
1. Implement fixes for Bugs #1, #2, #3
2. Run full test suite
3. Test manually with all backend types (PostgreSQL, Spatialite, OGR)
4. Release as v3.0.5
5. Monitor for regressions over 1-2 weeks
6. Plan Bug #4 refactor for v3.1.0

---

**Document prepared by:** Claude Code
**Date:** January 7, 2026
**For review by:** FilterMate Development Team
